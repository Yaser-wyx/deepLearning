{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 搭建单个神经元进行训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "CIFAR_DIR = \"../data/cifar-10-batches-py\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Batches.Meta\nData_Batch_1\nData_Batch_2\nData_Batch_3\nData_Batch_4\nData_Batch_5\nReadme.Html\nTest_Batch\n../data/cifar-10-batches-py\\Data_Batch_1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for file in os.listdir(CIFAR_DIR):\n",
    "    print(file.title())\n",
    "print(os.path.join(CIFAR_DIR,os.listdir(CIFAR_DIR)[1].title()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with open(os.path.join(CIFAR_DIR,os.listdir(CIFAR_DIR)[1].title()),\"rb\") as ff:\n",
    "    data = pickle.load(ff,encoding=\"bytes\")\n",
    "    print(data.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(10000, 3072) (10000,)\n",
      "(2000, 3072) (2000,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"read data from data file\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f, encoding = 'bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "class CifarData:\n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        \"\"\"从文件中读取cifar数据\"\"\"\n",
    "        for filename in filenames:\n",
    "            f_data, f_labels = load_data(filename)\n",
    "            for data, label in zip(f_data, f_labels):\n",
    "                if label in [0, 1]:\n",
    "                    all_data.append(data)\n",
    "                    all_labels.append(label)       \n",
    "        self._data = np.vstack(all_data)\n",
    "        # 进行归一化处理\n",
    "        self._data = self._data / 127.5 - 1\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        print(self._data.shape, self._labels.shape)\n",
    "        self._num_examples = len(self._data)\n",
    "        self._indicator = 0\n",
    "        self._need_shuffle = need_shuffle\n",
    "        if need_shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def _shuffle_data(self):\n",
    "        index = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[index]\n",
    "        self._labels = self._labels[index]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        if batch_size > self._num_examples:\n",
    "            raise Exception(\"the batch size is lager than all examples\")\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "train_filenames = [os.path.join(CIFAR_DIR, \"Data_Batch_%d\" % i)  \n",
    "                   for i in range(1, 6)]\n",
    "test_filename = [os.path.join(CIFAR_DIR, \"Test_Batch\")]\n",
    "\n",
    "train_data = CifarData(train_filenames, True)\n",
    "test_data = CifarData(test_filename, False)\n",
    "\n",
    "# batch_data, batch_labels = train_data.next_batch(10)\n",
    "# print(batch_data)\n",
    "# print(batch_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From j:\\anacoda\\envs\\tf_2g\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from tensorflow import compat as tf_compat\n",
    "tf = tf_compat.v1\n",
    "tf.disable_v2_behavior()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 搭建单个神经元"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 3072]) # 输入神经元的值\n",
    "y = tf.placeholder(tf.int64,[None]) # 输出标签\n",
    "\n",
    "# 每一个神经元输入的权重\n",
    "w = tf.get_variable('w', [x.get_shape()[-1], 1], \n",
    "                    initializer = tf.random_normal_initializer(0, 1))\n",
    "# 偏置节点的值\n",
    "b = tf.get_variable('b', [1], initializer = tf.constant_initializer(0.0))\n",
    "# 将输入的值经过权值处理后\n",
    "y_ = tf.matmul(x, w) + b\n",
    "# 将输入的值进行sigmoid处理\n",
    "y_p_1 = tf.nn.sigmoid(y_)\n",
    "# 将y值进行reshape处理\n",
    "y_reshape = tf.reshape(y, [-1, 1])\n",
    "# 转换int为float类型                                             \n",
    "y_reshape_float = tf.cast(y_reshape, tf.float32)\n",
    "loss = tf.reduce_mean(tf.square(y_reshape_float - y_p_1))\n",
    "\n",
    "predict = y_p_1 > 0.5\n",
    "correct_prediction = tf.equal(tf.cast(predict, tf.int64), y_reshape)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[Train] Step 500, loss: 0.24872, acc: 0.75000\n",
      "[Train] Step 1000, loss: 0.15005, acc: 0.85000\n",
      "[Train] Step 1500, loss: 0.26409, acc: 0.70000\n",
      "[Train] Step 2000, loss: 0.24489, acc: 0.75000\n(2000, 3072) (2000,)\n[Test] Step: 2000, acc: 0.76050\n",
      "[Train] Step 2500, loss: 0.15000, acc: 0.85000\n",
      "[Train] Step 3000, loss: 0.25000, acc: 0.75000\n",
      "[Train] Step 3500, loss: 0.34950, acc: 0.65000\n",
      "[Train] Step 4000, loss: 0.05000, acc: 0.95000\n(2000, 3072) (2000,)\n[Test] Step: 4000, acc: 0.79050\n",
      "[Train] Step 4500, loss: 0.35090, acc: 0.65000\n",
      "[Train] Step 5000, loss: 0.34490, acc: 0.65000\n",
      "[Train] Step 5500, loss: 0.11056, acc: 0.90000\n",
      "[Train] Step 6000, loss: 0.29645, acc: 0.70000\n(2000, 3072) (2000,)\n[Test] Step: 6000, acc: 0.79700\n",
      "[Train] Step 6500, loss: 0.38203, acc: 0.60000\n",
      "[Train] Step 7000, loss: 0.20231, acc: 0.80000\n",
      "[Train] Step 7500, loss: 0.14979, acc: 0.85000\n",
      "[Train] Step 8000, loss: 0.19981, acc: 0.80000\n(2000, 3072) (2000,)\n[Test] Step: 8000, acc: 0.80100\n",
      "[Train] Step 8500, loss: 0.27670, acc: 0.70000\n",
      "[Train] Step 9000, loss: 0.20000, acc: 0.80000\n",
      "[Train] Step 9500, loss: 0.05506, acc: 0.95000\n",
      "[Train] Step 10000, loss: 0.22914, acc: 0.75000\n(2000, 3072) (2000,)\n[Test] Step: 10000, acc: 0.80950",
      "\n",
      "[Train] Step 10500, loss: 0.24991, acc: 0.75000\n",
      "[Train] Step 11000, loss: 0.20000, acc: 0.80000\n",
      "[Train] Step 11500, loss: 0.30000, acc: 0.70000\n",
      "[Train] Step 12000, loss: 0.20000, acc: 0.80000\n(2000, 3072) (2000,)\n[Test] Step: 12000, acc: 0.80700\n",
      "[Train] Step 12500, loss: 0.22387, acc: 0.75000\n",
      "[Train] Step 13000, loss: 0.25009, acc: 0.75000\n",
      "[Train] Step 13500, loss: 0.29707, acc: 0.70000\n",
      "[Train] Step 14000, loss: 0.15107, acc: 0.85000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 14000, acc: 0.81200\n",
      "[Train] Step 14500, loss: 0.15352, acc: 0.85000\n",
      "[Train] Step 15000, loss: 0.30359, acc: 0.65000\n",
      "[Train] Step 15500, loss: 0.17353, acc: 0.80000\n",
      "[Train] Step 16000, loss: 0.23222, acc: 0.70000\n(2000, 3072) (2000,)\n[Test] Step: 16000, acc: 0.81350",
      "\n",
      "[Train] Step 16500, loss: 0.31382, acc: 0.65000\n",
      "[Train] Step 17000, loss: 0.30269, acc: 0.70000\n",
      "[Train] Step 17500, loss: 0.18511, acc: 0.75000\n",
      "[Train] Step 18000, loss: 0.15818, acc: 0.85000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 18000, acc: 0.81250\n",
      "[Train] Step 18500, loss: 0.06201, acc: 0.90000\n",
      "[Train] Step 19000, loss: 0.14980, acc: 0.85000\n",
      "[Train] Step 19500, loss: 0.27415, acc: 0.70000\n",
      "[Train] Step 20000, loss: 0.15019, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 20000, acc: 0.81350\n",
      "[Train] Step 20500, loss: 0.09778, acc: 0.90000\n",
      "[Train] Step 21000, loss: 0.18983, acc: 0.75000\n",
      "[Train] Step 21500, loss: 0.21485, acc: 0.75000\n",
      "[Train] Step 22000, loss: 0.06095, acc: 0.95000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 22000, acc: 0.81600\n",
      "[Train] Step 22500, loss: 0.29991, acc: 0.70000\n",
      "[Train] Step 23000, loss: 0.29923, acc: 0.70000\n",
      "[Train] Step 23500, loss: 0.25003, acc: 0.75000\n",
      "[Train] Step 24000, loss: 0.16432, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 24000, acc: 0.81600\n",
      "[Train] Step 24500, loss: 0.14867, acc: 0.85000\n",
      "[Train] Step 25000, loss: 0.15001, acc: 0.85000\n",
      "[Train] Step 25500, loss: 0.05000, acc: 0.95000\n",
      "[Train] Step 26000, loss: 0.10000, acc: 0.90000\n(2000, 3072) (2000,)\n[Test] Step: 26000, acc: 0.81800",
      "\n",
      "[Train] Step 26500, loss: 0.19998, acc: 0.80000\n",
      "[Train] Step 27000, loss: 0.10006, acc: 0.90000\n",
      "[Train] Step 27500, loss: 0.29113, acc: 0.70000\n",
      "[Train] Step 28000, loss: 0.22954, acc: 0.75000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 28000, acc: 0.81600\n",
      "[Train] Step 28500, loss: 0.15000, acc: 0.85000\n",
      "[Train] Step 29000, loss: 0.05059, acc: 0.95000\n",
      "[Train] Step 29500, loss: 0.20443, acc: 0.80000\n",
      "[Train] Step 30000, loss: 0.05296, acc: 0.95000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 30000, acc: 0.81850\n",
      "[Train] Step 30500, loss: 0.05000, acc: 0.95000\n",
      "[Train] Step 31000, loss: 0.20001, acc: 0.80000\n",
      "[Train] Step 31500, loss: 0.05002, acc: 0.95000\n",
      "[Train] Step 32000, loss: 0.10073, acc: 0.90000\n(2000, 3072) (2000,)\n[Test] Step: 32000, acc: 0.81750\n",
      "[Train] Step 32500, loss: 0.30000, acc: 0.70000\n",
      "[Train] Step 33000, loss: 0.14859, acc: 0.85000\n",
      "[Train] Step 33500, loss: 0.10037, acc: 0.90000\n",
      "[Train] Step 34000, loss: 0.07960, acc: 0.90000\n(2000, 3072) (2000,)\n[Test] Step: 34000, acc: 0.81950\n",
      "[Train] Step 34500, loss: 0.10099, acc: 0.90000\n",
      "[Train] Step 35000, loss: 0.05067, acc: 0.95000\n",
      "[Train] Step 35500, loss: 0.18514, acc: 0.80000\n",
      "[Train] Step 36000, loss: 0.33076, acc: 0.65000\n(2000, 3072) (2000,)\n[Test] Step: 36000, acc: 0.81850",
      "\n",
      "[Train] Step 36500, loss: 0.15026, acc: 0.85000\n",
      "[Train] Step 37000, loss: 0.10000, acc: 0.90000\n",
      "[Train] Step 37500, loss: 0.04672, acc: 0.95000\n",
      "[Train] Step 38000, loss: 0.05009, acc: 0.95000\n(2000, 3072) (2000,)\n[Test] Step: 38000, acc: 0.82400",
      "\n",
      "[Train] Step 38500, loss: 0.20252, acc: 0.80000\n",
      "[Train] Step 39000, loss: 0.15000, acc: 0.85000\n",
      "[Train] Step 39500, loss: 0.20000, acc: 0.80000\n",
      "[Train] Step 40000, loss: 0.15000, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 40000, acc: 0.82250\n",
      "[Train] Step 40500, loss: 0.00004, acc: 1.00000\n",
      "[Train] Step 41000, loss: 0.19284, acc: 0.80000\n",
      "[Train] Step 41500, loss: 0.15001, acc: 0.85000\n",
      "[Train] Step 42000, loss: 0.16429, acc: 0.80000\n(2000, 3072) (2000,)\n[Test] Step: 42000, acc: 0.82100\n",
      "[Train] Step 42500, loss: 0.25021, acc: 0.75000\n",
      "[Train] Step 43000, loss: 0.15136, acc: 0.85000\n",
      "[Train] Step 43500, loss: 0.00000, acc: 1.00000\n",
      "[Train] Step 44000, loss: 0.18204, acc: 0.80000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 44000, acc: 0.82050\n",
      "[Train] Step 44500, loss: 0.14999, acc: 0.85000\n",
      "[Train] Step 45000, loss: 0.20000, acc: 0.80000\n",
      "[Train] Step 45500, loss: 0.20044, acc: 0.80000\n",
      "[Train] Step 46000, loss: 0.05241, acc: 0.95000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 46000, acc: 0.82250\n",
      "[Train] Step 46500, loss: 0.23783, acc: 0.75000\n",
      "[Train] Step 47000, loss: 0.14992, acc: 0.85000\n",
      "[Train] Step 47500, loss: 0.25000, acc: 0.75000\n",
      "[Train] Step 48000, loss: 0.20003, acc: 0.80000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 48000, acc: 0.82050\n",
      "[Train] Step 48500, loss: 0.19969, acc: 0.80000\n",
      "[Train] Step 49000, loss: 0.00000, acc: 1.00000\n",
      "[Train] Step 49500, loss: 0.35000, acc: 0.65000\n",
      "[Train] Step 50000, loss: 0.18465, acc: 0.80000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 50000, acc: 0.82200\n",
      "[Train] Step 50500, loss: 0.05009, acc: 0.95000\n",
      "[Train] Step 51000, loss: 0.05023, acc: 0.95000\n",
      "[Train] Step 51500, loss: 0.10086, acc: 0.90000\n",
      "[Train] Step 52000, loss: 0.14975, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 52000, acc: 0.82350",
      "\n",
      "[Train] Step 52500, loss: 0.15000, acc: 0.85000\n",
      "[Train] Step 53000, loss: 0.15221, acc: 0.85000\n",
      "[Train] Step 53500, loss: 0.19982, acc: 0.80000\n",
      "[Train] Step 54000, loss: 0.10116, acc: 0.90000\n(2000, 3072) (2000,)\n[Test] Step: 54000, acc: 0.82250\n",
      "[Train] Step 54500, loss: 0.10918, acc: 0.90000\n",
      "[Train] Step 55000, loss: 0.24998, acc: 0.75000\n",
      "[Train] Step 55500, loss: 0.10222, acc: 0.90000\n",
      "[Train] Step 56000, loss: 0.15477, acc: 0.85000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 56000, acc: 0.82500\n",
      "[Train] Step 56500, loss: 0.20237, acc: 0.80000\n",
      "[Train] Step 57000, loss: 0.10200, acc: 0.90000\n",
      "[Train] Step 57500, loss: 0.20372, acc: 0.80000\n",
      "[Train] Step 58000, loss: 0.05005, acc: 0.95000\n(2000, 3072) (2000,)\n[Test] Step: 58000, acc: 0.82250\n",
      "[Train] Step 58500, loss: 0.15003, acc: 0.85000\n",
      "[Train] Step 59000, loss: 0.05281, acc: 0.95000\n",
      "[Train] Step 59500, loss: 0.14998, acc: 0.85000\n",
      "[Train] Step 60000, loss: 0.14999, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 60000, acc: 0.82250",
      "\n",
      "[Train] Step 60500, loss: 0.10171, acc: 0.90000\n",
      "[Train] Step 61000, loss: 0.12476, acc: 0.85000\n",
      "[Train] Step 61500, loss: 0.19977, acc: 0.80000\n",
      "[Train] Step 62000, loss: 0.20000, acc: 0.80000\n(2000, 3072) (2000,)\n[Test] Step: 62000, acc: 0.82100",
      "\n",
      "[Train] Step 62500, loss: 0.25005, acc: 0.75000\n",
      "[Train] Step 63000, loss: 0.10003, acc: 0.90000\n",
      "[Train] Step 63500, loss: 0.19995, acc: 0.80000\n",
      "[Train] Step 64000, loss: 0.10001, acc: 0.90000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 64000, acc: 0.82250\n",
      "[Train] Step 64500, loss: 0.15193, acc: 0.85000\n",
      "[Train] Step 65000, loss: 0.20065, acc: 0.80000\n",
      "[Train] Step 65500, loss: 0.15096, acc: 0.85000\n",
      "[Train] Step 66000, loss: 0.10006, acc: 0.90000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 66000, acc: 0.82250\n",
      "[Train] Step 66500, loss: 0.10011, acc: 0.90000\n",
      "[Train] Step 67000, loss: 0.19986, acc: 0.80000\n",
      "[Train] Step 67500, loss: 0.30030, acc: 0.70000\n",
      "[Train] Step 68000, loss: 0.15009, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 68000, acc: 0.82400",
      "\n",
      "[Train] Step 68500, loss: 0.25110, acc: 0.75000\n",
      "[Train] Step 69000, loss: 0.14994, acc: 0.85000\n",
      "[Train] Step 69500, loss: 0.07077, acc: 0.90000\n",
      "[Train] Step 70000, loss: 0.15000, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 70000, acc: 0.82350",
      "\n",
      "[Train] Step 70500, loss: 0.20020, acc: 0.80000\n",
      "[Train] Step 71000, loss: 0.00002, acc: 1.00000\n",
      "[Train] Step 71500, loss: 0.20078, acc: 0.80000\n",
      "[Train] Step 72000, loss: 0.25638, acc: 0.75000\n(2000, 3072) (2000,)\n[Test] Step: 72000, acc: 0.82600\n",
      "[Train] Step 72500, loss: 0.16349, acc: 0.85000\n",
      "[Train] Step 73000, loss: 0.09997, acc: 0.90000\n",
      "[Train] Step 73500, loss: 0.41515, acc: 0.55000\n",
      "[Train] Step 74000, loss: 0.10002, acc: 0.90000\n(2000, 3072) (2000,)\n[Test] Step: 74000, acc: 0.82250",
      "\n",
      "[Train] Step 74500, loss: 0.25036, acc: 0.75000\n",
      "[Train] Step 75000, loss: 0.05012, acc: 0.95000\n",
      "[Train] Step 75500, loss: 0.10001, acc: 0.90000\n",
      "[Train] Step 76000, loss: 0.25000, acc: 0.75000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 76000, acc: 0.82350\n",
      "[Train] Step 76500, loss: 0.09797, acc: 0.90000\n",
      "[Train] Step 77000, loss: 0.25000, acc: 0.75000\n",
      "[Train] Step 77500, loss: 0.10001, acc: 0.90000\n",
      "[Train] Step 78000, loss: 0.05214, acc: 0.95000\n(2000, 3072) (2000,)\n[Test] Step: 78000, acc: 0.82400\n",
      "[Train] Step 78500, loss: 0.15012, acc: 0.85000\n",
      "[Train] Step 79000, loss: 0.14866, acc: 0.85000\n",
      "[Train] Step 79500, loss: 0.14722, acc: 0.85000\n",
      "[Train] Step 80000, loss: 0.15049, acc: 0.85000\n(2000, 3072) (2000,)\n[Test] Step: 80000, acc: 0.82500\n",
      "[Train] Step 80500, loss: 0.15748, acc: 0.85000\n",
      "[Train] Step 81000, loss: 0.05570, acc: 0.95000\n",
      "[Train] Step 81500, loss: 0.20003, acc: 0.80000\n",
      "[Train] Step 82000, loss: 0.20001, acc: 0.80000\n(2000, 3072) (2000,)\n[Test] Step: 82000, acc: 0.82350\n",
      "[Train] Step 82500, loss: 0.09072, acc: 0.90000\n",
      "[Train] Step 83000, loss: 0.10004, acc: 0.90000\n",
      "[Train] Step 83500, loss: 0.10033, acc: 0.90000\n",
      "[Train] Step 84000, loss: 0.15361, acc: 0.85000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 84000, acc: 0.82300\n",
      "[Train] Step 84500, loss: 0.12547, acc: 0.85000\n",
      "[Train] Step 85000, loss: 0.06915, acc: 0.95000\n",
      "[Train] Step 85500, loss: 0.00006, acc: 1.00000\n",
      "[Train] Step 86000, loss: 0.15216, acc: 0.85000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 86000, acc: 0.82600\n",
      "[Train] Step 86500, loss: 0.15010, acc: 0.85000\n",
      "[Train] Step 87000, loss: 0.25220, acc: 0.75000\n",
      "[Train] Step 87500, loss: 0.11230, acc: 0.90000\n",
      "[Train] Step 88000, loss: 0.10030, acc: 0.90000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 88000, acc: 0.82650\n",
      "[Train] Step 88500, loss: 0.00004, acc: 1.00000\n",
      "[Train] Step 89000, loss: 0.25001, acc: 0.75000\n",
      "[Train] Step 89500, loss: 0.05394, acc: 0.95000\n",
      "[Train] Step 90000, loss: 0.11105, acc: 0.90000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 90000, acc: 0.82550\n",
      "[Train] Step 90500, loss: 0.14999, acc: 0.85000\n",
      "[Train] Step 91000, loss: 0.15965, acc: 0.85000\n",
      "[Train] Step 91500, loss: 0.05886, acc: 0.95000\n",
      "[Train] Step 92000, loss: 0.25000, acc: 0.75000\n(2000, 3072) (2000,)\n[Test] Step: 92000, acc: 0.82650\n",
      "[Train] Step 92500, loss: 0.10817, acc: 0.90000\n",
      "[Train] Step 93000, loss: 0.10002, acc: 0.90000\n",
      "[Train] Step 93500, loss: 0.20000, acc: 0.80000\n",
      "[Train] Step 94000, loss: 0.10275, acc: 0.90000\n(2000, 3072) (2000,)\n[Test] Step: 94000, acc: 0.82400\n",
      "[Train] Step 94500, loss: 0.05057, acc: 0.95000\n",
      "[Train] Step 95000, loss: 0.15017, acc: 0.85000\n",
      "[Train] Step 95500, loss: 0.04999, acc: 0.95000\n",
      "[Train] Step 96000, loss: 0.20011, acc: 0.80000\n(2000, 3072) (2000,)\n[Test] Step: 96000, acc: 0.82550\n",
      "[Train] Step 96500, loss: 0.11813, acc: 0.85000\n",
      "[Train] Step 97000, loss: 0.17300, acc: 0.80000\n",
      "[Train] Step 97500, loss: 0.10001, acc: 0.90000\n",
      "[Train] Step 98000, loss: 0.25084, acc: 0.75000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 98000, acc: 0.82450\n",
      "[Train] Step 98500, loss: 0.09710, acc: 0.90000\n",
      "[Train] Step 99000, loss: 0.10014, acc: 0.90000\n",
      "[Train] Step 99500, loss: 0.16717, acc: 0.80000\n",
      "[Train] Step 100000, loss: 0.06064, acc: 0.95000\n(2000, 3072) (2000,)\n",
      "[Test] Step: 100000, acc: 0.82600\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps = 100\n",
    "with tf.Session() as sess:\n",
    "    # init\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, acc_val, _ = sess.run(\n",
    "            [loss, accuracy, train_op], \n",
    "            feed_dict = {\n",
    "                x: batch_data,\n",
    "                y: batch_labels\n",
    "            })\n",
    "        # print training log\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(\"[Train] Step %d, loss: %4.5f, acc: %4.5f\"  \n",
    "                  % (i + 1, loss_val, acc_val))\n",
    "        # test the model\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            test_data = CifarData(test_filename, False)\n",
    "            test_acc_all = []\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels \\\n",
    "                    = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run(\n",
    "                    [accuracy], \n",
    "                    feed_dict = {\n",
    "                        x: test_batch_data,\n",
    "                        y: test_batch_labels\n",
    "                    })\n",
    "                test_acc_all.append(test_acc_val)\n",
    "            test_acc = np.mean(test_acc_all)\n",
    "            print(\"[Test] Step: %d, acc: %4.5f\" % (i + 1, test_acc ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}